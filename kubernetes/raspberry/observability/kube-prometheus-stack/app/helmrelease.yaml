---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app kube-prometheus-stack
  namespace: &namespace observability
spec:
  interval: 1h
  timeout: 30m
  chartRef:
    kind: OCIRepository
    name: kube-prometheus-stack
    namespace: flux-system
  install:
    crds: CreateReplace
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      strategy: rollback
      retries: 3
  values:
    crds:
      upgradeJob:
        enabled: true
        forceConflicts: true
    alertmanager:
      ingress:
        enabled: true
        pathType: Prefix
        ingressClassName: internal
        annotations:
          gethomepage.dev/enabled: "true"
          gethomepage.dev/group: Monitoring
          gethomepage.dev/icon: alertmanager.png
          gethomepage.dev/name: Alertmanager
          gethomepage.dev/app: alertmanager
        hosts:
          - &host alertmanager.${SECRET_DOMAIN}
        tls:
          - hosts:
              - *host
      alertmanagerSpec:
        replicas: 1
        alertmanagerConfiguration:
          name: alertmanager-config
    prometheus:
      service:
        type: LoadBalancer
        loadBalancerIP: 192.168.0.35
      ingress:
        enabled: true
        ingressClassName: internal
        pathType: Prefix
        annotations:
          gethomepage.dev/enabled: "true"
          gethomepage.dev/app: kube-prometheus-stack-prometheus-operator
          gethomepage.dev/group: Monitoring
          gethomepage.dev/icon: prometheus.png
          gethomepage.dev/name: Prometheus
          gethomepage.dev/widget.type: prometheus
          gethomepage.dev/widget.fields: "[\"targets_up\", \"targets_down\"]"
          gethomepage.dev/widget.url: http://kube-prometheus-stack-prometheus.observability:9090
        hosts:
          - &host prometheus.${SECRET_DOMAIN}
        tls:
          - hosts:
              - *host
      serviceMonitor:
        relabelings:
          - replacement: raspberry
            targetLabel: cluster
      prometheusSpec:
        replicas: 1
        scrapeInterval: 30s
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        scrapeConfigSelectorNilUsesHelmValues: false
        enableAdminAPI: true
        walCompression: true
        enableFeatures:
          - auto-gomaxprocs
          - memory-snapshot-on-shutdown
          - new-service-discovery-manager
          - out-of-order-time-series
        enableRemoteWriteReceiver: true
        tsdb:
          outOfOrderTimeWindow: 5m
        retention: 30d
        retentionSize: 128GiB
        # Drop high-cardinality metrics to reduce storage
        metricRelabelConfigs:
          # Drop expensive histogram buckets for API server
          - sourceLabels: [__name__]
            regex: 'apiserver_request_duration_seconds_bucket|apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket'
            action: drop
          # Drop high-cardinality container metrics
          - sourceLabels: [__name__]
            regex: 'container_network_tcp_usage_total|container_network_udp_usage_total'
            action: drop
          # Drop verbose etcd metrics
          - sourceLabels: [__name__]
            regex: 'etcd_request_duration_seconds_bucket|etcd_disk_wal_fsync_duration_seconds_bucket|etcd_disk_backend_commit_duration_seconds_bucket'
            action: drop
          # Drop high-cardinality workqueue metrics
          - sourceLabels: [__name__]
            regex: 'workqueue_queue_duration_seconds_bucket|workqueue_work_duration_seconds_bucket'
            action: drop
          # Drop rest client request latency buckets
          - sourceLabels: [__name__]
            regex: 'rest_client_request_duration_seconds_bucket'
            action: drop
        nodeSelector:
          disktype: ssd
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn
              resources:
                requests:
                  storage: 128Gi
    kubeApiServer:
      serviceMonitor:
        metricRelabelings:
          - sourceLabels: [__name__]
            action: drop
            regex: '(apiserver|etcd)_request_duration_seconds_bucket'
        relabelings:
          - replacement: raspberry
            targetLabel: cluster
    kubelet:
      serviceMonitor:
        cAdvisorRelabelings:
          - sourceLabels: [__metrics_path__]
            targetLabel: metrics_path
          - replacement: raspberry
            targetLabel: cluster
        cAdvisorMetricRelabelings:
          # Drop expensive cAdvisor histogram buckets
          - sourceLabels: [__name__]
            regex: 'container_cpu_cfs_throttled_periods_total|container_cpu_cfs_periods_total'
            action: keep
          - sourceLabels: [__name__]
            regex: 'container_(network|memory|fs|cpu|spec).*'
            action: keep
          - sourceLabels: [__name__]
            action: drop
            regex: 'container_.*_bucket'
        relabelings:
          - sourceLabels: [__metrics_path__]
            targetLabel: metrics_path
          - replacement: raspberry
            targetLabel: cluster
    kubeControllerManager:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeProxy:
      enabled: false
    kube-state-metrics:
      prometheus:
        monitor:
          relabelings:
            - replacement: raspberry
              targetLabel: cluster
    prometheus-node-exporter:
      prometheus:
        monitor:
          relabelings:
            - replacement: raspberry
              targetLabel: cluster
    prometheusOperator:
      serviceMonitor:
        relabelings:
          - replacement: raspberry
            targetLabel: cluster
    grafana:
      enabled: true
      defaultDashboardsTimezone: ${TIMEZONE}
      deploymentStrategy:
        type: Recreate
      extraConfigmapMounts:
        - name: grafana-custom-ini
          mountPath: /etc/grafana/custom.ini
          subPath: custom.ini
          configMap: grafana-custom-ini
          readOnly: true
      ingress:
        enabled: true
        ingressClassName: internal
        annotations:
          gethomepage.dev/enabled: "true"
          gethomepage.dev/app: grafana
          gethomepage.dev/group: Monitoring
          gethomepage.dev/icon: grafana.png
          gethomepage.dev/name: Grafana
        hosts:
          - &host grafana.${SECRET_DOMAIN}
        tls:
          - hosts:
              - *host
      sidecar:
        dashboards:
          multicluster:
            global:
              enabled: true
          provider:
            foldersFromFilesStructure: true
          searchNamespace: observability
          folderAnnotation: grafana_folder
      persistence:
        enabled: true
        storageClassName: longhorn
        accessModes:
          - ReadWriteOnce
        size: 8Gi
        labels:
          recurring-job-group.longhorn.io/backup: enabled
      datasources:
        datasources.yaml:
          apiVersion: 1
          datasources:
            - name: Prometheus
              type: prometheus
              uid: prometheus
              url: http://kube-prometheus-stack-prometheus.observability:9090
              access: proxy
              jsonData:
                timeInterval: 30s
            - name: Alertmanager
              type: alertmanager
              uid: alertmanager
              url: http://kube-prometheus-stack-alertmanager.observability.svc.cluster.local:9093
              access: proxy
              jsonData:
                implementation: prometheus
      nodeSelector:
        disktype: ssd
